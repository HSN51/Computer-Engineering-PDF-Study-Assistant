# app.py (BaÅŸlangÄ±Ã§)
import streamlit as st
from dotenv import load_dotenv
import os
from langchain_community.document_loaders import PyMuPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain.chains.question_answering import load_qa_chain
from langchain.prompts import PromptTemplate
from langchain_community.callbacks import get_openai_callback
import numpy as np

# --- 0. GENEL AYARLAR VE YÃœKLEMELER ---
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
LLM_MODEL_NAME = "gpt-3.5-turbo"
EMBEDDINGS_MODEL_NAME = "text-embedding-3-small"

# --- 1. PDF Ä°ÅLEME FONKSÄ°YONLARI ---
def process_pdfs(pdf_files):
    """
    YÃ¼klenen PDF dosyalarÄ±nÄ± okur, metinlerini Ã§Ä±karÄ±r ve LangChain Document nesnelerine dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r.
    """
    all_docs_content = []
    for pdf_file in pdf_files:
        temp_file_path = os.path.join(".", pdf_file.name)
        with open(temp_file_path, "wb") as f:
            f.write(pdf_file.getbuffer())
        loader = PyMuPDFLoader(temp_file_path)
        documents = loader.load()
        all_docs_content.extend(documents)
        os.remove(temp_file_path)
    return all_docs_content

def split_text_into_chunks(documents):
    """
    LangChain Document listesini anlamlÄ± parÃ§alara (chunks) bÃ¶ler.
    """
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=900,
        chunk_overlap=150,
        length_function=len
    )
    chunks = text_splitter.split_documents(documents)
    return chunks

# --- 2. EMBEDDING TABANLI CHUNK SEÃ‡Ä°MÄ° ---
@st.cache_resource
def get_embeddings_model():
    return OpenAIEmbeddings(model=EMBEDDINGS_MODEL_NAME, openai_api_key=OPENAI_API_KEY)

def embed_chunks(chunks, embeddings_model):
    texts = [chunk.page_content for chunk in chunks]
    vectors = embeddings_model.embed_documents(texts)
    return np.array(vectors)

def embed_query(query, embeddings_model):
    return np.array(embeddings_model.embed_query(query))

def find_relevant_chunks(query, chunks, embeddings_model, chunk_vectors, k=5):
    if not chunks or chunk_vectors is None:
        return []
    query_vec = embed_query(query, embeddings_model)
    sims = np.dot(chunk_vectors, query_vec) / (np.linalg.norm(chunk_vectors, axis=1) * np.linalg.norm(query_vec) + 1e-8)
    top_indices = np.argsort(sims)[-k:][::-1]
    return [chunks[i] for i in top_indices]

# --- 3. SORU CEVAPLAMA MEKANÄ°ZMASI ---
def get_llm():
    return ChatOpenAI(model_name=LLM_MODEL_NAME, temperature=0.2, openai_api_key=OPENAI_API_KEY)

def generate_answer_with_llm(query, relevant_chunks, llm):
    if not relevant_chunks:
        return "ÃœzgÃ¼nÃ¼m, bu soruya cevap verecek ilgili bilgi ders notlarÄ±nÄ±zda bulunamadÄ±."
    prompt_template_text = """
Sen bir Computer Engineering alanÄ±nda uzman bir asistan ve sÄ±nav koÃ§usun. KullanÄ±cÄ±dan gelen SORU'yu, sadece aÅŸaÄŸÄ±daki BAÄLAMdaki bilgilere dayanarak, vize/final sÄ±navÄ±na hazÄ±rlanan bir Ã¶ÄŸrenciye veya konuyu hÄ±zlÄ±ca Ã¶ÄŸrenmek isteyen birine yÃ¶nelik, kÄ±sa, Ã¶z, teknik ve anlaÅŸÄ±lÄ±r bir dille yanÄ±tla.

- Gereksiz detay, referans veya akademik dil kullanma.
- Sadece Ã¶zet, anahtar noktalar, formÃ¼ller, temel kavramlar ve sÄ±navda iÅŸine yarayacak pÃ¼f noktalarÄ±nÄ± vurgula.
- Gerekirse Ã¶rnek, kÄ±sa aÃ§Ä±klama veya formÃ¼l ekle.
- CevabÄ±n Computer Engineering alanÄ±nda profesyonel ve pratik olsun.

BAÄLAM:
{context}

SORU: {question}

CEVAP (KÄ±sa, Ã¶z, teknik, sÄ±nav odaklÄ±):"""
    CUSTOM_PROMPT = PromptTemplate(
        template=prompt_template_text, input_variables=["context", "question"]
    )
    chain = load_qa_chain(llm, chain_type="stuff", prompt=CUSTOM_PROMPT)
    with get_openai_callback() as cb:
        try:
            response = chain.invoke({"input_documents": relevant_chunks, "question": query})
            answer = response.get('output_text', "Cevap alÄ±nÄ±rken bir sorun oluÅŸtu veya 'output_text' anahtarÄ± bulunamadÄ±.")
            st.session_state.token_usage = {
                "total_tokens": cb.total_tokens,
                "prompt_tokens": cb.prompt_tokens,
                "completion_tokens": cb.completion_tokens,
                "total_cost_usd": cb.total_cost
            }
        except Exception as e:
            st.error(f"LLM'den cevap alÄ±nÄ±rken hata: {e}")
            return "Cevap Ã¼retilirken bir sorun oluÅŸtu."
    return answer

def generate_summary(chunks, llm, summary_type="kÄ±sa"):
    context = "\n".join([chunk.page_content for chunk in chunks[:8]])  # ilk 8 chunk (yaklaÅŸÄ±k ilk 6-8 sayfa)
    prompt = f"""
AÅŸaÄŸÄ±da Computer Engineering alanÄ±nda bir ders notundan alÄ±nmÄ±ÅŸ metinler var. Bu metinleri {summary_type} ve sÄ±nav/pratik odaklÄ±, teknik bir Ã¶zet haline getir. Gereksiz detay verme, anahtar noktalarÄ± vurgula.

METÄ°N:
{context}

Ã–ZET ({summary_type}):
"""
    with get_openai_callback() as cb:
        try:
            response = llm.invoke(prompt)
            return response.content
        except Exception as e:
            st.error(f"Ã–zet Ã¼retilirken hata: {e}")
            return "Ã–zet Ã¼retilemedi."

def generate_flashcards(chunks, llm):
    context = "\n".join([chunk.page_content for chunk in chunks[:10]])
    prompt = f"""
AÅŸaÄŸÄ±da Computer Engineering alanÄ±nda bir ders notundan alÄ±nmÄ±ÅŸ metinler var. Bu metinlerden sÄ±nav/pratik odaklÄ±, teknik 5 adet flashcard (soru-cevap kartÄ±) Ã¼ret. Her kartta kÄ±sa bir soru ve kÄ±sa bir teknik cevap olsun. Sadece kartlarÄ± listele.

METÄ°N:
{context}

FLASHCARD LÄ°STESÄ°:
"""
    with get_openai_callback() as cb:
        try:
            response = llm.invoke(prompt)
            return response.content
        except Exception as e:
            st.error(f"Flashcard Ã¼retilirken hata: {e}")
            return "Flashcard Ã¼retilemedi."

def generate_quiz(chunks, llm):
    context = "\n".join([chunk.page_content for chunk in chunks[:10]])
    prompt = f"""
AÅŸaÄŸÄ±da Computer Engineering alanÄ±nda bir ders notundan alÄ±nmÄ±ÅŸ metinler var. Bu metinlerden sÄ±nav/pratik odaklÄ±, teknik 3 adet Ã§oktan seÃ§meli quiz sorusu Ã¼ret. Her sorunun 1 doÄŸru, 3 yanlÄ±ÅŸ ÅŸÄ±kkÄ± olsun. Sadece quiz sorularÄ±nÄ± ve ÅŸÄ±klarÄ±nÄ± listele.

METÄ°N:
{context}

QUIZ SORULARI:
"""
    with get_openai_callback() as cb:
        try:
            response = llm.invoke(prompt)
            return response.content
        except Exception as e:
            st.error(f"Quiz Ã¼retilirken hata: {e}")
            return "Quiz Ã¼retilemedi."

# --- DÃ–KÃœMAN Ä°Ã‡Ä° ARAMA VE SAYFA ATLAMA ---
def search_in_document(query, chunks):
    results = []
    for i, chunk in enumerate(chunks):
        if query.lower() in chunk.page_content.lower():
            page = chunk.metadata.get('page', i+1) if hasattr(chunk, 'metadata') else i+1
            results.append({
                'chunk_index': i,
                'page': page,
                'content': chunk.page_content
            })
    return results

# --- MODERN ARAYÃœZ VE TEMA ---
def set_modern_theme():
    st.markdown(
        """
        <style>
        .stApp {background-color: #f7f9fa;}
        .block-container {padding-top: 2rem;}
        .stButton>button {background-color: #0056b3; color: white; border-radius: 6px; font-weight: 600;}
        .stButton>button:hover {background-color: #003d80;}
        .stTextInput>div>input {border-radius: 6px; border: 1.5px solid #0056b3;}
        .stSidebar {background-color: #e9ecef;}
        .stInfo, .stSuccess, .stWarning {border-radius: 8px;}
        .stChatMessage {border-radius: 8px;}
        </style>
        """,
        unsafe_allow_html=True
    )

# --- 4. STREAMLIT KULLANICI ARAYÃœZÃœ ---
def main():
    st.set_page_config(page_title="Akademik Soru-Cevap Chatbotu", page_icon="ğŸ’»", layout="wide")
    st.header("ğŸ’» Computer Engineering SÄ±nav KoÃ§u ve AkÄ±llÄ± PDF Chatbotu")
    st.markdown("""
    Bu chatbot, yÃ¼klediÄŸiniz PDF ders notlarÄ±nÄ±z Ã¼zerinden **Computer Engineering** alanÄ±nda, vize/final ve pratik Ã¶ÄŸrenme odaklÄ±, profesyonel ve hÄ±zlÄ± cevaplar vermek iÃ§in tasarlanmÄ±ÅŸtÄ±r.
    PDF'inizi yÃ¼kleyin, ardÄ±ndan teknik, sÄ±nav odaklÄ± ve anlaÅŸÄ±lÄ±r cevaplar alÄ±n.
    """)

    if not OPENAI_API_KEY:
        st.error("LÃ¼tfen .env dosyasÄ±nda OPENAI_API_KEY deÄŸiÅŸkenini ayarlayÄ±n!")
        return

    set_modern_theme()

    with st.sidebar:
        st.subheader("ğŸ“„ Ders NotlarÄ±nÄ±zÄ± YÃ¼kleyin")
        uploaded_files = st.file_uploader(
            "PDF dosyalarÄ±nÄ±zÄ± buraya sÃ¼rÃ¼kleyip bÄ±rakÄ±n veya seÃ§in",
            type="pdf",
            accept_multiple_files=True,
            key="pdf_uploader"
        )
        if uploaded_files:
            st.success(f"{len(uploaded_files)} PDF yÃ¼klendi. Åimdi soru sorabilirsiniz.")
            st.markdown("---")
            if st.button("Otomatik Ã–zet Ãœret", key="summary_btn"):
                with st.spinner("Ã–zet Ã¼retiliyor..."):
                    raw_documents = process_pdfs(uploaded_files)
                    chunks = split_text_into_chunks(raw_documents)
                    llm = get_llm()
                    summary = generate_summary(chunks, llm, summary_type="kÄ±sa")
                    st.session_state.generated_summary = summary
            if st.button("Flashcard Ãœret", key="flashcard_btn"):
                with st.spinner("Flashcard Ã¼retiliyor..."):
                    raw_documents = process_pdfs(uploaded_files)
                    chunks = split_text_into_chunks(raw_documents)
                    llm = get_llm()
                    flashcards = generate_flashcards(chunks, llm)
                    st.session_state.generated_flashcards = flashcards
            if st.button("Quiz Ãœret", key="quiz_btn"):
                with st.spinner("Quiz Ã¼retiliyor..."):
                    raw_documents = process_pdfs(uploaded_files)
                    chunks = split_text_into_chunks(raw_documents)
                    llm = get_llm()
                    quiz = generate_quiz(chunks, llm)
                    st.session_state.generated_quiz = quiz
            st.markdown("---")
            st.subheader("ğŸ” DÃ¶kÃ¼man Ä°Ã§i Arama")
            search_query = st.text_input("Anahtar kelime ile ara...", key="doc_search")
            if uploaded_files and search_query:
                raw_documents = process_pdfs(uploaded_files)
                chunks = split_text_into_chunks(raw_documents)
                search_results = search_in_document(search_query, chunks)
                if search_results:
                    st.write(f"{len(search_results)} sonuÃ§ bulundu:")
                    for res in search_results:
                        if st.button(f"Sayfa {res['page']} - Sonucu GÃ¶ster", key=f"show_{res['chunk_index']}"):
                            st.session_state.selected_search_result = res
                else:
                    st.info("AradÄ±ÄŸÄ±nÄ±z kelimeyle eÅŸleÅŸen iÃ§erik bulunamadÄ±.")
            st.markdown("---")
        st.markdown("---")
        if "token_usage" in st.session_state and st.session_state.token_usage:
            usage = st.session_state.token_usage
            st.subheader("Son Sorgu API KullanÄ±mÄ±:")
            st.text(f"Toplam Token: {usage['total_tokens']}")
            st.text(f"Ä°stem TokenlarÄ±: {usage['prompt_tokens']}")
            st.text(f"Cevap TokenlarÄ±: {usage['completion_tokens']}")
            st.text(f"Tahmini Maliyet: ${usage['total_cost_usd']:.6f}")
            st.session_state.token_usage = None

    if "messages" not in st.session_state:
        st.session_state.messages = [{"role": "assistant", "content": "Merhaba! Computer Engineering alanÄ±nda, PDF'inizden sÄ±nav ve pratik odaklÄ± teknik sorular sorabilirsiniz. LÃ¼tfen Ã¶nce soldaki menÃ¼den PDF'lerinizi yÃ¼kleyin."}]

    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    query = st.chat_input("Sorunuzu buraya yazÄ±n...", key="chat_input")

    if query:
        st.session_state.messages.append({"role": "user", "content": query})
        with st.chat_message("user"):
            st.markdown(query)
        with st.chat_message("assistant"):
            message_placeholder = st.empty()
            full_response = ""
            if not uploaded_files:
                full_response = "LÃ¼tfen Ã¶nce sol taraftan PDF dosyalarÄ±nÄ±zÄ± yÃ¼kleyin."
                message_placeholder.markdown(full_response)
                st.session_state.messages.append({"role": "assistant", "content": full_response})
                st.stop()
            with st.spinner("Cevap aranÄ±yor ve oluÅŸturuluyor..."):
                try:
                    raw_documents = process_pdfs(uploaded_files)
                    chunks = split_text_into_chunks(raw_documents)
                    embeddings_model = get_embeddings_model()
                    chunk_vectors = embed_chunks(chunks, embeddings_model)
                    relevant_chunks = find_relevant_chunks(query, chunks, embeddings_model, chunk_vectors)
                    llm = get_llm()
                    full_response = generate_answer_with_llm(query, relevant_chunks, llm)
                    with st.expander("ğŸ“„ Cevap iÃ§in KullanÄ±lan Kaynaklar (Ders Notu Kesitleri)"):
                        for i, chunk in enumerate(relevant_chunks):
                            st.markdown(f"**Kaynak {i+1}:**")
                            st.caption(chunk.page_content[:500] + "..." if len(chunk.page_content) > 500 else chunk.page_content)
                            if hasattr(chunk, 'metadata') and chunk.metadata:
                                st.json(chunk.metadata, expanded=False)
                except Exception as e:
                    st.error(f"Cevap alÄ±nÄ±rken bir hata oluÅŸtu: {e}")
                    full_response = "ÃœzgÃ¼nÃ¼m, bir sorunla karÅŸÄ±laÅŸtÄ±m ve cevap Ã¼retemedim."
            message_placeholder.markdown(full_response)
        st.session_state.messages.append({"role": "assistant", "content": full_response})

    if "generated_summary" in st.session_state:
        st.info("### Otomatik Ã–zet:\n" + st.session_state.generated_summary)
    if "generated_flashcards" in st.session_state:
        st.success("### Flashcard KartlarÄ±:\n" + st.session_state.generated_flashcards)
    if "generated_quiz" in st.session_state:
        st.warning("### Quiz SorularÄ±:\n" + st.session_state.generated_quiz)

    if "selected_search_result" in st.session_state:
        res = st.session_state.selected_search_result
        st.info(f"**Sayfa {res['page']} - Arama Sonucu:**\n" + res['content'])

if __name__ == "__main__":
    main()